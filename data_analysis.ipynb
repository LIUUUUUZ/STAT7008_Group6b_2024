{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from utilities import DataPreProcessor\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dpp = DataPreProcessor()\n",
    "dpp.load_data(download=True)\n",
    "dpp.show_data_info(3, only_head=True)\n",
    "\n",
    "df  = dpp.get_data()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "null_percentages = df.isnull().mean() * 100\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for column, percentage in null_percentages.items():\n",
    "\n",
    "  if percentage:\n",
    "\n",
    "    print({column: [round(percentage, 5), df[column].dtype]})\n",
    "\n",
    "# Get the value counts for 'application_type'\n",
    "application_type_counts = df['application_type'].value_counts()\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(application_type_counts.values, labels=application_type_counts.index, autopct='%1.1f%%', \n",
    "        startangle=90, colors=sns.color_palette('Set2'))\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(application_type_counts.index, title=\"Application Types\", bbox_to_anchor=(1, 1), loc=\"best\")\n",
    "\n",
    "# Set title\n",
    "plt.title('Distribution of Application Types')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the percentage of missing values for the 'JOINT' application type\n",
    "joint_na_percentages = df[df['application_type'] == 'JOINT'].isna().mean() * 100\n",
    "print(joint_na_percentages) \n",
    "# Filter columns with more than 5% missing values\n",
    "joint_na_percentages = joint_na_percentages[joint_na_percentages > 5]\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Create the bar plot using Seaborn\n",
    "sns.barplot(x=joint_na_percentages.values, y=joint_na_percentages.index, palette='Set2')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Percentage of Missing Values')\n",
    "plt.ylabel('Column')\n",
    "plt.title('Percentage of Missing Values for JOINT Application Type')\n",
    "\n",
    "# Customize plot and background color\n",
    "plt.gca().set_facecolor('lightgray')  # Background color inside the plot\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the percentage of missing values for the 'INDIVIDUAL' application type\n",
    "individual_na_percentages = df[df['application_type'] == 'INDIVIDUAL'].isna().mean() * 100\n",
    "print(individual_na_percentages)\n",
    "\n",
    "# Filter columns with more than 5% missing values\n",
    "individual_na_percentages = individual_na_percentages[individual_na_percentages > 5]\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the bar plot using Seaborn\n",
    "sns.barplot(x=individual_na_percentages.values, y=individual_na_percentages.index, palette='Set2')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Percentage of Missing Values')\n",
    "plt.ylabel('Column')\n",
    "plt.title('Percentage of Missing Values for INDIVIDUAL Application Type')\n",
    "\n",
    "# Customize plot and background color\n",
    "plt.gca().set_facecolor('lightgray')  # Background color inside the plot\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# precossing the data, drop JOINT records to keep all values for INDIVIDUAL application_type\n",
    "# since the JOINT application_type is not similar to the INDIVIDUAL application_type and has relatively low instances\n",
    "df = df[df['application_type'] != 'JOINT']\n",
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# processing the data, drop columns with 'joint' in the name and the 'application_type' column\n",
    "columns_to_drop = [i for i in df.columns if 'joint' in i]\n",
    "\n",
    "columns_to_drop.append('application_type')\n",
    "\n",
    "print(columns_to_drop)\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the percentage of missing values for the entire DataFrame\n",
    "na_percentages = df.isna().mean() * 100\n",
    "\n",
    "# Filter columns with more than 5% missing values\n",
    "na_percentages = na_percentages[na_percentages > 5]\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the bar plot using Seaborn\n",
    "sns.barplot(x=na_percentages.values, y=na_percentages.index, palette='Set2')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Percentage of Missing Values')\n",
    "plt.ylabel('Column')\n",
    "plt.title('Percentage of Missing Values in the Whole Data')\n",
    "\n",
    "# Customize plot and background color\n",
    "plt.gca().set_facecolor('lightgray')  # Background color inside the plot\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# processing the data, drop columns with more than 20% missing values\n",
    "columns_to_drop = []\n",
    "\n",
    "for i in df.columns:\n",
    "  \n",
    "  if df[i].isna().mean()*100 > 20:\n",
    "\n",
    "    columns_to_drop.append(i)\n",
    "\n",
    "print(columns_to_drop)\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "for i in df.columns:\n",
    "\n",
    "  print({i: df[i].nunique()})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# check the number of unique values less than 10\n",
    "for i in df.columns:\n",
    "\n",
    "    if df[i].nunique() < 10:\n",
    "\n",
    "        print({i: df[i].value_counts()})\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# processing the data, drop columns with only one unique value\n",
    "columns_to_drop = ['policy_code']\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.head(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df[['grade', 'sub_grade']].head(5))\n",
    "print(df['grade'].unique())\n",
    "sub_grades = df['sub_grade'].unique()\n",
    "\n",
    "sub_grades.sort()\n",
    "\n",
    "sub_grades"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# processing the data, drop the some columns\n",
    "df.drop(columns=['member_id', 'url', 'issue_d', 'earliest_cr_line', 'grade', 'last_credit_pull_d', 'pymnt_plan'], inplace=True)\n",
    "\n",
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# processing the data, drop the some columns by experience\n",
    "df.drop(columns=['emp_title', 'title', 'zip_code', 'addr_state', 'last_pymnt_d'], inplace=True)\n",
    "\n",
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set up the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the histogram using Seaborn\n",
    "sns.histplot(data=df, x=\"emp_length\", hue=\"loan_status\", multiple=\"dodge\", palette='Set2')\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title(\"Relationship between Employment Length and Loan Status\")\n",
    "plt.xlabel(\"Employment Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# processing the data, drop the 'emp_length' column, since it has no significant impact on the target variable\n",
    "df.drop(columns=['emp_length'], inplace=True)\n",
    "\n",
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "importante_features = df.columns\n",
    "len(importante_features)\n",
    "\n",
    "# handle missing values\n",
    "\n",
    "\n",
    "# Get the percentage of missing values for the entire DataFrame\n",
    "na_percentages = df.isna().mean() * 100\n",
    "\n",
    "# Filter columns with any missing values\n",
    "na_percentages = na_percentages[na_percentages > 0]\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the bar plot using Seaborn\n",
    "sns.barplot(y=na_percentages.values, x=na_percentages.index, palette='Set2')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Percentage of Missing Values')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Distribution of Nulls in Final Features')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels and align them to the right\n",
    "\n",
    "# Customize background color\n",
    "plt.gca().set_facecolor('lightgray')  # Background color inside the plot\n",
    "plt.gcf().set_facecolor('lightblue')  # Figure background color\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "df[na_percentages.index].info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df.shape)\n",
    "\n",
    "# processing the data, drop rows with missing values less than 1%\n",
    "\n",
    "for i in na_percentages.index:\n",
    "\n",
    "  if na_percentages[i] < 1:\n",
    "\n",
    "    df.dropna(subset=[i], inplace=True)\n",
    "\n",
    "print(df.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "importante_features"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# plot all the features\n",
    "\n",
    "features = [col for col in df.columns if df[col].nunique() > 2]\n",
    "\n",
    "rows = (len(features) + 2) // 3\n",
    "\n",
    "cols = 3\n",
    "\n",
    "# Set up the figure and axis for subplots\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))\n",
    "\n",
    "axes = axes.flatten()  # Flatten in case of multiple rows\n",
    "\n",
    "\n",
    "fig.patch.set_facecolor('lightgray')  # Set the background color of the entire figure\n",
    "\n",
    "\n",
    "palette = sns.color_palette('Set2', len(features))\n",
    "\n",
    "\n",
    "plotted_df = df.copy()\n",
    "\n",
    "plotted_df['diff_loan_funded'] = plotted_df['loan_amnt'] - plotted_df['funded_amnt']\n",
    "\n",
    "for i, col in enumerate(features):\n",
    "\n",
    "    sns.histplot(x=plotted_df[col], kde=False, ax=axes[i], color=palette[i], alpha=1)  # Set alpha slightly transparent for better visualization\n",
    "\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "# Remove any empty subplots (in case the number of features doesn't fill the grid)\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.suptitle(\"Distributions of Features\", fontsize=16)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for the main title\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some further analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "avg_income = df.groupby('loan_status')['annual_inc'].mean().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.barplot(y='annual_inc',x='loan_status', data=avg_income)\n",
    "\n",
    "\n",
    "\n",
    "plt.title('Loan Status by Average Annual Income', fontsize=14)\n",
    "\n",
    "plt.xlabel('Loan Status', fontsize=12)\n",
    "\n",
    "plt.ylabel('Average Annual Income', fontsize=12)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "subgrade_counts = df['sub_grade'].value_counts().sort_index()\n",
    "\n",
    "\n",
    "\n",
    "subgrade_proportions = df.groupby(['sub_grade', 'loan_status']).size().unstack(fill_value=0)\n",
    "\n",
    "subgrade_proportions = subgrade_proportions.div(subgrade_proportions.sum(axis=1), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
    "\n",
    "ax1, ax2 = axes[0], axes[1]\n",
    "\n",
    "\n",
    "\n",
    "sns.barplot(x=subgrade_counts.index, y=subgrade_counts.values, ax=ax1, width=0.6)\n",
    "\n",
    "ax1.set_title('Number of Loans per Subgrade', fontsize=14)\n",
    "\n",
    "ax1.set_xlabel('Subgrade', fontsize=12)\n",
    "\n",
    "ax1.set_ylabel('Number of Loans', fontsize=12)\n",
    "\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "\n",
    "subgrade_proportions.plot(kind='bar', stacked=True, ax=ax2, color=sns.color_palette('Dark2'))\n",
    "\n",
    "ax2.set_title('Loan Status by Subgrade', fontsize=14)\n",
    "\n",
    "ax2.set_xlabel('Subgrade', fontsize=12)\n",
    "\n",
    "ax2.set_ylabel('Proportion', fontsize=12)\n",
    "\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax2.legend(title='Loan Status', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "purpose_status_counts = df.groupby(['purpose', 'loan_status']).size().unstack(fill_value=0)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "\n",
    "purpose_status_counts.plot(kind='bar', stacked=True, figsize=(12, 6), color=sns.color_palette('Dark2'))\n",
    "\n",
    "\n",
    "\n",
    "plt.title('Loan Status by Purpose', fontsize=14)\n",
    "\n",
    "plt.xlabel('Purpose of Loan', fontsize=12)\n",
    "\n",
    "plt.ylabel('Number of Loans', fontsize=12)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "categorical_features = df.select_dtypes(include='object').drop(columns=['loan_status'])\n",
    "categorical_features.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i in categorical_features.columns:\n",
    "\n",
    "  print({i: categorical_features[i].value_counts()})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def sub_grades_encoding(x):\n",
    "\n",
    "  val = 0\n",
    "\n",
    "  if 'A' in x:\n",
    "\n",
    "    val = 7\n",
    "\n",
    "  elif 'B' in x:\n",
    "\n",
    "    val = 6\n",
    "\n",
    "  elif 'C' in x:\n",
    "\n",
    "    val = 5\n",
    "\n",
    "  elif 'D' in x:\n",
    "\n",
    "    val = 4\n",
    "\n",
    "  elif 'E' in x:\n",
    "\n",
    "    val = 3\n",
    "\n",
    "  elif 'F' in x:\n",
    "\n",
    "    val = 2\n",
    "\n",
    "  elif 'G' in x:\n",
    "\n",
    "    val = 1\n",
    "\n",
    "\n",
    "\n",
    "  if '1' in x:\n",
    "\n",
    "    val += 0.8\n",
    "\n",
    "  elif '2' in x:\n",
    "\n",
    "    val += 0.6\n",
    "\n",
    "  elif '3' in x:\n",
    "\n",
    "    val += 0.4\n",
    "\n",
    "  elif '4' in x:\n",
    "\n",
    "    val += 0.2\n",
    "\n",
    "  elif '5' in x:\n",
    "\n",
    "    val += 0.0\n",
    "\n",
    "\n",
    "\n",
    "  return val\n",
    "\n",
    "\n",
    "\n",
    "def verification_status(x):\n",
    "\n",
    "  if x == 'Not Verified':\n",
    "\n",
    "    return 0\n",
    "\n",
    "  return 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['sub_grade'] = df['sub_grade'].apply(sub_grades_encoding)\n",
    "df['sub_grade'].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['term'] = label_encoder.fit_transform(df['term'])\n",
    "df['initial_list_status'] = label_encoder.fit_transform(df['initial_list_status'])\n",
    "df['verification_status'] = df['verification_status'].apply(verification_status)\n",
    "categorical_features = df.select_dtypes(include='object').drop(columns=['loan_status'])\n",
    "categorical_features.isna().sum()\n",
    "encoded_features = pd.get_dummies(categorical_features, dtype=int)\n",
    "\n",
    "encoded_features.head(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.concat([df, encoded_features], axis=1)\n",
    "df.drop(columns=categorical_features.columns, inplace=True)\n",
    "df.head(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def loan_status(x):\n",
    "    if x == 'Current':\n",
    "        return 0\n",
    "    elif x == 'Fully Paid':\n",
    "        return 1\n",
    "    elif x == 'Charged Off':\n",
    "        return 2\n",
    "    elif x == 'Late (31-120 days)':\n",
    "        return 3\n",
    "    elif x == 'Issued':\n",
    "        return 4\n",
    "    elif x == 'In Grace Period':\n",
    "        return 5\n",
    "    elif x == 'Late (16-30 days)':\n",
    "        return 6\n",
    "    elif x == 'Does not meet the credit policy. Status:Fully Paid':\n",
    "        return 7\n",
    "    elif x == 'Default':\n",
    "        return 8\n",
    "    elif x == 'Does not meet the credit policy. Status:Charged Off':\n",
    "        return 9\n",
    "    else:\n",
    "        raise ValueError('Unknown loan status: ' + x)\n",
    "\n",
    "# df['loan_status'] = label_encoder.fit_transform(df['loan_status'])\n",
    "print(df['loan_status'].value_counts())\n",
    "\n",
    "df['loan_status'] = df['loan_status'].apply(loan_status)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df['loan_status'].value_counts())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.isna().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_c = df.copy()\n",
    "blank_columns: list = df_c.columns[df_c.isna().any()].tolist()\n",
    "print(blank_columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def fill_blank(d, mode = 'mean') -> None:\n",
    "    assert mode in ['mean', 'mode', 'zero', 'remove'], 'mode should be either mean, mode or zero'\n",
    "    if mode == 'mean':\n",
    "        d['tot_coll_amt'].fillna(d['tot_coll_amt'].mean(), inplace=True)\n",
    "        d['tot_cur_bal'].fillna(d['tot_cur_bal'].mean(), inplace=True)\n",
    "        d['total_rev_hi_lim'].fillna(d['total_rev_hi_lim'].mean(), inplace=True)\n",
    "\n",
    "    elif mode == 'mode':\n",
    "        d['tot_coll_amt'].fillna(d['tot_coll_amt'].mode()[0], inplace=True)\n",
    "        d['tot_cur_bal'].fillna(d['tot_cur_bal'].mode()[0], inplace=True)\n",
    "        d['total_rev_hi_lim'].fillna(d['total_rev_hi_lim'].mode()[0], inplace=True)\n",
    "    \n",
    "    elif mode == 'zero':\n",
    "        d['tot_coll_amt'].fillna(0, inplace=True)\n",
    "        d['tot_cur_bal'].fillna(0, inplace=True)\n",
    "        d['total_rev_hi_lim'].fillna(0, inplace=True)\n",
    "\n",
    "    elif mode == 'remove':\n",
    "        d.dropna(subset=['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim'], inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df_c.shape)\n",
    "# test the function\n",
    "fill_blank(df_c, mode='remove')\n",
    "# print(df_c.isna().sum())\n",
    "# show value in the columns\n",
    "print(df_c['tot_coll_amt'].value_counts().head(3))\n",
    "print(df_c['tot_cur_bal'].value_counts().head(3))\n",
    "print(df_c['total_rev_hi_lim'].value_counts().head(3))\n",
    "\n",
    "print(df_c.shape)\n",
    "print(df_c.isna().sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "x = df_c.drop(columns=['loan_status'])\n",
    "\n",
    "y = df_c['loan_status']\n",
    "\n",
    "x_rest, X, y_rest, Y = train_test_split(x, y, test_size=0.1, random_state=42, stratify=y, shuffle=True)\n",
    "print(x_rest.shape, X.shape, y_rest.shape, Y.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y, shuffle=True)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit_transform(x)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from joblib import dump, load\n",
    "# dump(scaler, 'models/scaler.joblib')\n",
    "# load_scaler = load('models/scaler.joblib')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def maxmin_scaler(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    scaler: MinMaxScaler = load('models/scaler.joblib')\n",
    "    cols: pd.DataFrame = x.columns\n",
    "    x = scaler.transform(x)\n",
    "    x: pd.DataFrame = pd.DataFrame(x, columns=cols)\n",
    "    return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x_train = maxmin_scaler(x_train)\n",
    "x_test = maxmin_scaler(x_test)\n",
    "# print range\n",
    "print(x_test.head(30))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(x_train.head(30))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "features_selected = x_train.columns.to_list()[:15]\n",
    "print(features_selected)\n",
    "x_train = x_train[features_selected]\n",
    "x_test = x_test[features_selected]\n",
    "print(x_train.shape, x_test.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:23.029273Z",
     "start_time": "2024-10-31T09:29:18.915191Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install catboost",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /opt/anaconda3/lib/python3.12/site-packages (1.2.3)\r\n",
      "Requirement already satisfied: graphviz in /opt/anaconda3/lib/python3.12/site-packages (from catboost) (0.20.1)\r\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from catboost) (3.8.4)\r\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/anaconda3/lib/python3.12/site-packages (from catboost) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/anaconda3/lib/python3.12/site-packages (from catboost) (2.2.2)\r\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from catboost) (1.13.1)\r\n",
      "Requirement already satisfied: plotly in /opt/anaconda3/lib/python3.12/site-packages (from catboost) (5.22.0)\r\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from catboost) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2023.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->catboost) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->catboost) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->catboost) (4.51.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->catboost) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->catboost) (23.2)\r\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->catboost) (10.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->catboost) (3.0.9)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly->catboost) (8.2.2)\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "catboost测试"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:57.929022Z",
     "start_time": "2024-10-31T09:29:39.010215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utilities import DataPreProcessor\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "\n",
    "dpp = DataPreProcessor()\n",
    "dpp.load_data(download=True)\n",
    "dpp.preprocess_data(fill_blank=True, fill_mode='mode')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangzhe/Documents/GitHub/STAT7008_Group6b_2024/utilities/data_processor.py:128: DtypeWarning: Columns (19,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data: pd.DataFrame = pd.read_csv(self.input_path, index_col=0)\n",
      "/Users/zhangzhe/Documents/GitHub/STAT7008_Group6b_2024/utilities/data_processor.py:206: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  self.data[i].fillna(self.data[i].mode()[0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing Summary:\n",
      "Rows Deleted: 1126\n",
      "\t511 rows deleted due to del application_type 'JOINT'\n",
      "\t615 rows deleted due to del rows with insignificant missing values\n",
      "Columns Deleted: 37\n",
      "\t4 columns deleted due to del columns related to 'joint', namely: annual_inc_joint, dti_joint, verification_status_joint, application_type\n",
      "\t19 columns deleted due to del columns with more than 20% missing values, namely: desc, mths_since_last_delinq, mths_since_last_record, next_pymnt_d, mths_since_last_major_derog, open_acc_6m, open_il_6m, open_il_12m, open_il_24m, mths_since_rcnt_il, total_bal_il, il_util, open_rv_12m, open_rv_24m, max_bal_bc, all_util, inq_fi, total_cu_tl, inq_last_12m\n",
      "\t1 columns deleted due to del columns 'policy_code'\n",
      "\t13 columns deleted due to del columns after analyzing (analyzing procedures are in the file 'data_analysis.ipynb'), namely: member_id, url, issue_d, earliest_cr_line, grade, last_credit_pull_d, emp_title, title, zip_code, addr_state, last_pymnt_d, pymnt_plan, emp_length\n",
      "Blank values filled with mode, relative columns: ['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utilities import DataLoader, maxmin_scaler\n",
    "dl = DataLoader(dpp.get_data())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:40:53.655994Z",
     "start_time": "2024-10-31T09:40:53.641333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_catboost(self):\n",
    "    # 步骤 1: 编码数据（针对 CatBoost）\n",
    "    self.catboost_encoder()\n",
    "    \n",
    "    # 步骤 2: 分割数据\n",
    "    self.catboost_split_data()\n",
    "    \n",
    "    # 步骤 3: 特征选择（使用 CatBoost 的特征重要性）\n",
    "    selected_features = self.catboost_feature_selection()\n",
    "    \n",
    "    # 步骤 4: 训练 CatBoost 模型\n",
    "    self.train_catboost_model(selected_features)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def catboost_encoder(self) -> None:\n",
    "    if self.encodered:\n",
    "        print('Data already encoded')\n",
    "        return\n",
    "    # 自定义映射\n",
    "    self.data['sub_grade'] = self.data['sub_grade'].apply(sub_grades_encoding)\n",
    "    self.data['verification_status'] = self.data['verification_status'].apply(verification_status)\n",
    "    \n",
    "    # 标签编码\n",
    "    label_encoder = LabelEncoder()\n",
    "    self.data['term'] = label_encoder.fit_transform(self.data['term'])\n",
    "    self.data['initial_list_status'] = label_encoder.fit_transform(self.data['initial_list_status'])\n",
    "    \n",
    "    # 不进行独热编码，保留原始类别特征\n",
    "    # 将目标变量进行编码\n",
    "    self.data['loan_status'] = self.data['loan_status'].apply(loan_status)\n",
    "    \n",
    "    self.encodered: bool = True\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def catboost_split_data(self, dataset_size=0.1, test_size=0.3, random=True) -> tuple:\n",
    "    if self.splited:\n",
    "        print('Data already split')\n",
    "        return\n",
    "    if not self.encodered and not self.filled:\n",
    "        print('Data not encoded or filled, please encode and fill the data first')\n",
    "        return\n",
    "    x = self.data.drop(columns=['loan_status'])\n",
    "    y = self.data['loan_status']\n",
    "    x_rest, self.X, y_rest, self.Y = train_test_split(\n",
    "        x, y, test_size=dataset_size, random_state=1 if random else None)\n",
    "    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(\n",
    "        self.X, self.Y, test_size=test_size, random_state=1 if random else None, stratify=self.Y, shuffle=True)\n",
    "    self.splited = True\n",
    "    \n",
    "    # 确保类别特征的数据类型为字符串\n",
    "    categorical_cols = self.x_train.select_dtypes(exclude=[np.number]).columns\n",
    "    for col in categorical_cols:\n",
    "        self.x_train[col] = self.x_train[col].astype(str)\n",
    "        self.x_test[col] = self.x_test[col].astype(str)\n",
    "    \n",
    "    # 对数值型特征进行归一化\n",
    "    numeric_features = self.x_train.select_dtypes(include=[np.number]).columns\n",
    "    scaler = MinMaxScaler()\n",
    "    self.x_train[numeric_features] = scaler.fit_transform(self.x_train[numeric_features])\n",
    "    self.x_test[numeric_features] = scaler.transform(self.x_test[numeric_features])\n",
    "    \n",
    "    return self.x_train, self.x_test, self.y_train, self.y_test\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def catboost_feature_selection(self) -> list:\n",
    "    # 确保数据已分割\n",
    "    if not self.splited:\n",
    "        print('Data not split, please split the data first')\n",
    "        return []\n",
    "    \n",
    "    # 获取类别特征的列名\n",
    "    categorical_features = self.x_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # 定义数据池\n",
    "    train_pool = Pool(data=self.x_train, label=self.y_train, cat_features=categorical_features)\n",
    "    \n",
    "    # 初始化并训练模型\n",
    "    model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=False)\n",
    "    model.fit(train_pool)\n",
    "    \n",
    "    # 获取特征重要性\n",
    "    importances = model.get_feature_importance()\n",
    "    feature_names = self.x_train.columns\n",
    "    feature_importance = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    \n",
    "    # 按重要性排序，选择前15个特征\n",
    "    feature_importance.sort_values(by='importance', ascending=False, inplace=True)\n",
    "    selected_features = feature_importance.head(15)['feature'].tolist()\n",
    "    \n",
    "    print(f\"Selected features ({len(selected_features)}): {selected_features}\")\n",
    "    \n",
    "    # 更新过滤后的数据\n",
    "    self.x_train_filtered = self.x_train[selected_features]\n",
    "    self.x_test_filtered = self.x_test[selected_features]\n",
    "    self.filtered = True\n",
    "    \n",
    "    return [self.x_train_filtered, self.x_test_filtered]\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
